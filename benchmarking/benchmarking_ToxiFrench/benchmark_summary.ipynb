{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a203dcb",
   "metadata": {},
   "source": [
    "# Summary of the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e30488",
   "metadata": {},
   "source": [
    "The results can be compared with [this article](https://arxiv.org/pdf/2403.08035) in which GPT-3.5 achieves $\\text{acc}=0.89$ and $F_1=0.93$ in *English Hate Speech Detection*. We use newer models in this benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb68075",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Our task is a binary classification. We aim at reaching a ground truth $f:E\\to\\{0,1\\}$ which we will estimate with $\\hat f:E\\to\\{0,1\\}$.\n",
    "\n",
    "### Confusion Matrix Notations (Binary Classification)\n",
    "\n",
    "|              | Predicted 0 | Predicted 1 |\n",
    "| ------------ | ----------- | ----------- |\n",
    "| **Actual 0** | TN          | FP          |\n",
    "| **Actual 1** | FN          | TP          |\n",
    "\n",
    "* **TP** = True Positives (predicted 1, actual 1)\n",
    "* **TN** = True Negatives (predicted 0, actual 0)\n",
    "* **FP** = False Positives (predicted 1, actual 0)\n",
    "* **FN** = False Negatives (predicted 0, actual 1)\n",
    "\n",
    "### Metrics \n",
    "\n",
    "- **Precision\\_0** : The proportion of predicted class 0 that is actually class 0 : $$\\text{Precision}\\_0 = \\frac{\\text{TN}}{\\text{TN} + \\text{FN}}$$\n",
    "- **Recall\\_0** : The proportion of actual class 0 correctly predicted as class 0 : $$\\text{Recall}\\_0 = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}$$\n",
    "- **F1\\_0** : The harmonic mean of Precision\\_0 and Recall\\_0 : $$\\text{F1}_0 = 2 \\cdot \\frac{\\text{Precision}\\_0 \\times \\text{Recall}\\_0}{\\text{Precision}\\_0 + \\text{Recall}\\_0}$$\n",
    "- **Precision\\_1** : The proportion of predicted class 1 that is actually class 1 : $$\\text{Precision}\\_1 = \\frac{\\text{TN}}{\\text{TN} + \\text{FN}}$$\n",
    "- **Recall\\_1** : The proportion of actual class 1 correctly predicted as class 1 : $$\\text{Recall}\\_1 = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}$$\n",
    "- **F1\\_1** : The harmonic mean of Precision\\_1 and Recall\\_1 : $$\\text{F1}_0 = 2 \\cdot \\frac{\\text{Precision}\\_1 \\times \\text{Recall}\\_1}{\\text{Precision}\\_1 + \\text{Recall}\\_1}$$\n",
    "- **Accuracy** : The proportion of all correct predictions : $$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$$\n",
    "- **ROC AUC** : Area under the **Receiver Operating Characteristic** curve. It evaluates the tradeoff between True Positive Rate (TPR) and False Positive Rate (FPR) over all thresholds : $$\\text{TPR (Recall)} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}, \\quad \n",
    "\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fce313",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "407caa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import os\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736e7f7",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed44c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path('../..')\n",
    "DATA_PATH = ROOT / \"data\"\n",
    "BENCHMARK_PATH = DATA_PATH / \"benchmark\"\n",
    "\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73b774",
   "metadata": {},
   "source": [
    "## Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cffbb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclude = [\n",
    "    \"benchmark.csv\",\n",
    "    \"benchmark_balanced_subset.csv\",\n",
    "    \"checkpoint\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8744c715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded benchmark with <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1388</span> entries.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded benchmark with \u001b[1;36m1388\u001b[0m entries.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_benchmark = pd.read_csv(BENCHMARK_PATH / \"benchmark_balanced_subset.csv\", encoding = 'utf-8')\n",
    "len_benchmark = len(df_benchmark)\n",
    "console.print(f\"Loaded benchmark with {len_benchmark} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c8024b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34</span> additional files from benchmark directory.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded \u001b[1;36m34\u001b[0m additional files from benchmark directory.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for file in os.listdir(BENCHMARK_PATH):\n",
    "    if file.endswith('.csv') and all(file_exclude not in file for file_exclude in to_exclude):\n",
    "        file_path = BENCHMARK_PATH / file\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        df['file'] = file  \n",
    "        assert len(df) == len_benchmark, f\"Length mismatch for {file}: {len(df)} vs {len_benchmark}\"\n",
    "        assert \"prediction\" in df.columns, f\"'prediction' column missing in {file}\"\n",
    "        assert \"label\" in df.columns, f\"'prediction' column missing in {file}\"\n",
    "        dfs.append(df)\n",
    "\n",
    "console.print(f\"Loaded {len(dfs)} additional files from benchmark directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a088d5",
   "metadata": {},
   "source": [
    "## Compute the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a8630eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for df in dfs:\n",
    "    file_name = df['file'].iloc[0]\n",
    "    y_true = df['label'].astype(int)\n",
    "    y_pred = df['prediction']\n",
    "    row = {\"Model\": file_name.replace('.csv', '').replace('_', ' ')}\n",
    "\n",
    "    try:\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        row.update({\n",
    "            \"Precision_0\": report['0']['precision'],\n",
    "            \"Recall_0\": report['0']['recall'],\n",
    "            \"F1_0\": report['0']['f1-score'],\n",
    "            \"Precision_1\": report['1']['precision'],\n",
    "            \"Recall_1\": report['1']['recall'],\n",
    "            \"F1_1\": report['1']['f1-score'],\n",
    "            \"Accuracy\": report['accuracy'],\n",
    "        })\n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]Error computing classification report for {file_name}: {e}[/red]\")\n",
    "\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "        row[\"ROC_AUC\"] = roc_auc\n",
    "    except:\n",
    "        row[\"ROC_AUC\"] = None\n",
    "\n",
    "    results.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cab93c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                         Benchmark Summary for All Models                                          </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">          Model           </span>┃<span style=\"font-weight: bold\"> Precision_0 </span>┃<span style=\"font-weight: bold\"> Recall_0 </span>┃<span style=\"font-weight: bold\"> F1_0  </span>┃<span style=\"font-weight: bold\"> Precision_1 </span>┃<span style=\"font-weight: bold\"> Recall_1 </span>┃<span style=\"font-weight: bold\"> F1_1  </span>┃<span style=\"font-weight: bold\"> Accuracy </span>┃<span style=\"font-weight: bold\"> ROC_AUC </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│       gpt-4o-mini        │    0.962    │  0.771   │ 0.856 │    0.809    │  0.970   │ 0.882 │  0.870   │  0.870  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│     gemini-2.0-flash     │    0.954    │  0.769   │ 0.852 │    0.807    │  0.963   │ 0.878 │  0.866   │  0.866  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│      mistral medium      │    0.924    │  0.787   │ 0.850 │    0.814    │  0.935   │ 0.871 │  0.861   │  0.861  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│    gpt-4o-mini simple    │    0.959    │  0.746   │ 0.840 │    0.792    │  0.968   │ 0.872 │  0.857   │  0.857  │\n",
       "│          prompt          │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│      mistral large       │    0.953    │  0.752   │ 0.841 │    0.795    │  0.963   │ 0.871 │  0.857   │  0.857  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│      gemini-1.5-pro      │    0.942    │  0.755   │ 0.838 │    0.796    │  0.954   │ 0.868 │  0.854   │  0.854  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│          gpt-4o          │    0.990    │  0.696   │ 0.817 │    0.766    │  0.993   │ 0.864 │  0.844   │  0.844  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│    deepseek-reasoner     │    0.967    │  0.709   │ 0.818 │    0.770    │  0.976   │ 0.861 │  0.842   │  0.842  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│      gpt-3.5-turbo       │    0.899    │  0.767   │ 0.827 │    0.796    │  0.914   │ 0.851 │  0.840   │  0.840  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│      deepseek-chat       │    0.983    │  0.683   │ 0.806 │    0.757    │  0.988   │ 0.858 │  0.836   │  0.836  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│      mistral small       │    0.954    │  0.695   │ 0.804 │    0.760    │  0.967   │ 0.851 │  0.831   │  0.831  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│  mistral medium simple   │    0.971    │  0.679   │ 0.799 │    0.753    │  0.980   │ 0.852 │  0.829   │  0.829  │\n",
       "│          prompt          │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│ gemini-2.0-flash simple  │    0.971    │  0.671   │ 0.794 │    0.749    │  0.980   │ 0.849 │  0.826   │  0.826  │\n",
       "│          prompt          │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│         o4-mini          │    0.862    │  0.767   │ 0.812 │    0.790    │  0.878   │ 0.831 │  0.822   │  0.822  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│   Qwen-2.5-3B-Instruct   │    0.829    │  0.810   │ 0.819 │    0.814    │  0.833   │ 0.823 │  0.821   │  0.821  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│    mistral moderation    │    0.809    │  0.837   │ 0.823 │    0.831    │  0.803   │ 0.817 │  0.820   │  0.820  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│   Qwen-2.5-7B-Instruct   │    0.917    │  0.702   │ 0.795 │    0.758    │  0.937   │ 0.838 │  0.819   │  0.819  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│     gemini-2.5-flash     │    0.952    │  0.625   │ 0.755 │    0.721    │  0.968   │ 0.827 │  0.797   │  0.797  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│   Qwen-2.5-3B-Instruct   │    0.881    │  0.673   │ 0.763 │    0.735    │  0.909   │ 0.813 │  0.791   │  0.791  │\n",
       "│      simple prompt       │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│       shieldgemma        │    0.842    │  0.677   │ 0.751 │    0.730    │  0.873   │ 0.795 │  0.775   │  0.775  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│   Qwen-2.5-7B-Instruct   │    0.922    │  0.595   │ 0.723 │    0.701    │  0.950   │ 0.807 │  0.772   │  0.772  │\n",
       "│      simple prompt       │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│        Qwen-3-4B         │    0.721    │  0.889   │ 0.796 │    0.855    │  0.656   │ 0.742 │  0.772   │  0.772  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│  openai omni moderation  │    0.760    │  0.781   │ 0.770 │    0.775    │  0.754   │ 0.764 │  0.767   │  0.767  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│   Mistral-7B-Instruct    │    0.735    │  0.729   │ 0.732 │    0.731    │  0.738   │ 0.735 │  0.733   │  0.733  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│        camembert         │    0.726    │  0.751   │ 0.738 │    0.742    │  0.716   │ 0.729 │  0.733   │  0.733  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│    perspective scores    │    0.638    │  0.903   │ 0.748 │    0.835    │  0.488   │ 0.616 │  0.696   │  0.696  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│        llamaguard        │    0.610    │  0.971   │ 0.749 │    0.929    │  0.379   │ 0.538 │  0.675   │  0.675  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│   Mistral-7B-Instruct    │    0.641    │  0.775   │ 0.702 │    0.716    │  0.566   │ 0.632 │  0.671   │  0.671  │\n",
       "│      simple prompt       │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│        toxic bert        │    0.724    │  0.549   │ 0.625 │    0.637    │  0.791   │ 0.706 │  0.670   │  0.670  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│       Qwen-3-1.7B        │    0.603    │  0.934   │ 0.733 │    0.853    │  0.385   │ 0.530 │  0.659   │  0.659  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│     french toxicity      │    0.667    │  0.624   │ 0.645 │    0.647    │  0.689   │ 0.667 │  0.656   │  0.656  │\n",
       "│        classifier        │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│ distilbert-base-multili… │    0.716    │  0.396   │ 0.510 │    0.583    │  0.843   │ 0.689 │  0.620   │  0.620  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│        polyguard         │    0.574    │  0.885   │ 0.696 │    0.748    │  0.343   │ 0.470 │  0.614   │  0.614  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│  Qwen-2.5-1.5B-Instruct  │    0.963    │  0.189   │ 0.316 │    0.550    │  0.993   │ 0.708 │  0.591   │  0.591  │\n",
       "└──────────────────────────┴─────────────┴──────────┴───────┴─────────────┴──────────┴───────┴──────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                         Benchmark Summary for All Models                                          \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         Model          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPrecision_0\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mRecall_0\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mF1_0 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPrecision_1\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mRecall_1\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mF1_1 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAccuracy\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mROC_AUC\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│       gpt-4o-mini        │    0.962    │  0.771   │ 0.856 │    0.809    │  0.970   │ 0.882 │  0.870   │  0.870  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│     gemini-2.0-flash     │    0.954    │  0.769   │ 0.852 │    0.807    │  0.963   │ 0.878 │  0.866   │  0.866  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│      mistral medium      │    0.924    │  0.787   │ 0.850 │    0.814    │  0.935   │ 0.871 │  0.861   │  0.861  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│    gpt-4o-mini simple    │    0.959    │  0.746   │ 0.840 │    0.792    │  0.968   │ 0.872 │  0.857   │  0.857  │\n",
       "│          prompt          │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│      mistral large       │    0.953    │  0.752   │ 0.841 │    0.795    │  0.963   │ 0.871 │  0.857   │  0.857  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│      gemini-1.5-pro      │    0.942    │  0.755   │ 0.838 │    0.796    │  0.954   │ 0.868 │  0.854   │  0.854  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│          gpt-4o          │    0.990    │  0.696   │ 0.817 │    0.766    │  0.993   │ 0.864 │  0.844   │  0.844  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│    deepseek-reasoner     │    0.967    │  0.709   │ 0.818 │    0.770    │  0.976   │ 0.861 │  0.842   │  0.842  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│      gpt-3.5-turbo       │    0.899    │  0.767   │ 0.827 │    0.796    │  0.914   │ 0.851 │  0.840   │  0.840  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│      deepseek-chat       │    0.983    │  0.683   │ 0.806 │    0.757    │  0.988   │ 0.858 │  0.836   │  0.836  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│      mistral small       │    0.954    │  0.695   │ 0.804 │    0.760    │  0.967   │ 0.851 │  0.831   │  0.831  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│  mistral medium simple   │    0.971    │  0.679   │ 0.799 │    0.753    │  0.980   │ 0.852 │  0.829   │  0.829  │\n",
       "│          prompt          │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│ gemini-2.0-flash simple  │    0.971    │  0.671   │ 0.794 │    0.749    │  0.980   │ 0.849 │  0.826   │  0.826  │\n",
       "│          prompt          │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│         o4-mini          │    0.862    │  0.767   │ 0.812 │    0.790    │  0.878   │ 0.831 │  0.822   │  0.822  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│   Qwen-2.5-3B-Instruct   │    0.829    │  0.810   │ 0.819 │    0.814    │  0.833   │ 0.823 │  0.821   │  0.821  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│    mistral moderation    │    0.809    │  0.837   │ 0.823 │    0.831    │  0.803   │ 0.817 │  0.820   │  0.820  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│   Qwen-2.5-7B-Instruct   │    0.917    │  0.702   │ 0.795 │    0.758    │  0.937   │ 0.838 │  0.819   │  0.819  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│     gemini-2.5-flash     │    0.952    │  0.625   │ 0.755 │    0.721    │  0.968   │ 0.827 │  0.797   │  0.797  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│   Qwen-2.5-3B-Instruct   │    0.881    │  0.673   │ 0.763 │    0.735    │  0.909   │ 0.813 │  0.791   │  0.791  │\n",
       "│      simple prompt       │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│       shieldgemma        │    0.842    │  0.677   │ 0.751 │    0.730    │  0.873   │ 0.795 │  0.775   │  0.775  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│   Qwen-2.5-7B-Instruct   │    0.922    │  0.595   │ 0.723 │    0.701    │  0.950   │ 0.807 │  0.772   │  0.772  │\n",
       "│      simple prompt       │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│        Qwen-3-4B         │    0.721    │  0.889   │ 0.796 │    0.855    │  0.656   │ 0.742 │  0.772   │  0.772  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│  openai omni moderation  │    0.760    │  0.781   │ 0.770 │    0.775    │  0.754   │ 0.764 │  0.767   │  0.767  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│   Mistral-7B-Instruct    │    0.735    │  0.729   │ 0.732 │    0.731    │  0.738   │ 0.735 │  0.733   │  0.733  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│        camembert         │    0.726    │  0.751   │ 0.738 │    0.742    │  0.716   │ 0.729 │  0.733   │  0.733  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│    perspective scores    │    0.638    │  0.903   │ 0.748 │    0.835    │  0.488   │ 0.616 │  0.696   │  0.696  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│        llamaguard        │    0.610    │  0.971   │ 0.749 │    0.929    │  0.379   │ 0.538 │  0.675   │  0.675  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│   Mistral-7B-Instruct    │    0.641    │  0.775   │ 0.702 │    0.716    │  0.566   │ 0.632 │  0.671   │  0.671  │\n",
       "│      simple prompt       │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│        toxic bert        │    0.724    │  0.549   │ 0.625 │    0.637    │  0.791   │ 0.706 │  0.670   │  0.670  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│       Qwen-3-1.7B        │    0.603    │  0.934   │ 0.733 │    0.853    │  0.385   │ 0.530 │  0.659   │  0.659  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│     french toxicity      │    0.667    │  0.624   │ 0.645 │    0.647    │  0.689   │ 0.667 │  0.656   │  0.656  │\n",
       "│        classifier        │             │          │       │             │          │       │          │         │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│ distilbert-base-multili… │    0.716    │  0.396   │ 0.510 │    0.583    │  0.843   │ 0.689 │  0.620   │  0.620  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│        polyguard         │    0.574    │  0.885   │ 0.696 │    0.748    │  0.343   │ 0.470 │  0.614   │  0.614  │\n",
       "├──────────────────────────┼─────────────┼──────────┼───────┼─────────────┼──────────┼───────┼──────────┼─────────┤\n",
       "│  Qwen-2.5-1.5B-Instruct  │    0.963    │  0.189   │ 0.316 │    0.550    │  0.993   │ 0.708 │  0.591   │  0.591  │\n",
       "└──────────────────────────┴─────────────┴──────────┴───────┴─────────────┴──────────┴───────┴──────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Convert to DataFrame & Display as Rich Table ===\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df = summary_df.sort_values(by=\"Accuracy\", ascending=False) \n",
    "\n",
    "# === Print Table in Rich ===\n",
    "rich_table = Table(title=\"Benchmark Summary for All Models\", show_lines=True)\n",
    "for col in summary_df.columns:\n",
    "    rich_table.add_column(col, justify=\"center\")\n",
    "\n",
    "for _, row in summary_df.iterrows():\n",
    "    rich_table.add_row(*[f\"{x:.3f}\" if isinstance(x, float) else str(x) for x in row])\n",
    "\n",
    "console.print(rich_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SJTU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
